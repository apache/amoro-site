[{"categories":null,"content":"Benchmark Report Test purpose This test aims at comparing the OLAP benchmark performance of various data lake formats in the scenario of continuous streaming ingestion in the CDC database.\nMeanwhile, particular attention was paid during the testing process to the impact of enabling self-optimizing on the analytical performance of the table.\nTest envrionment Hardware configuration Number OS Cpu core Memory Disk type Deployed components 1 CentOS 7 40 256 SAS HDFS、Hive、Yarn 2 CentOS 7 40 256 SAS HDFS、Hive、Yarn 3 CentOS 7 40 256 SAS HDFS、Hive、Yarn 4 CentOS 7 40 256 SAS Trino、Presto 5 CentOS 7 40 256 SAS Trino、Presto 6 CentOS 7 40 256 SAS Trino、Presto Software version Software Version Trino 380 Presto 274 Iceberg 0.13 Amoro 0.4 Hudi 0.11.1 Test plan Overview This test is based on CHbenchmark, which is a hybrid testing standard that integrates TPCC and TPCH. The overall testing workload can be divided into two categories:\n5 OLTP workloads based on TPC-C: NewOrder, Payment, OrderStatus, Delivery, and StockLevel.\n22 OLAP workloads based on TCP-H, where Q15 was abandoned in this test due to its association with views and the lack of view rewriting.\nPrepare test data Based on the TPC-C, the raw data was constructed in MySQL for this test. The dataset includes a total of 12 tables, with the relationship between TPC-C and TPC-H tables shown in the following diagram:\nIn addition, the relationship between the data sizes of each table is shown in the following table, where w represents the number of warehouses. It can be observed that the data sizes of intermediate tables such as new_order and stock are affected by the number of warehouses. Therefore, the data set size can be adjusted by controlling the number of warehouses during testing.\nIn this test, the number of warehouses was set to 100, and the initial data set size in the MySQL database was approximately 10GB. The following table shows the number of data records for each table in the initial data set and the changes in the number of data records for each table after running the one-hour TPC-C test.\nTable name The number of records (rows) in beginning The number of records (rows) after running a one-hour TPC-C test warehouse 100 100 item 100000 100000 stock 10000000 10000000 district 1000 1000 customer 3000000 3000000 history 3000000 3119285（+119285） oorder 3000000 3124142（+124142） new_order 893709 907373（+13664） order_line 29996774 31252799（+1256025） region 5 5 nation 62 62 supplier 1000 1000 Perform the test Before starting TPC-H testing, start a Flink job to synchronize both the history data and real-time data from MySQL into the data lake. Afterwards, execute TPC-H testing through query engines such as Trino or Presto.\nTPC-H contains 22 query statements, only three of which are listed here due to space limitation:\n-- query1 SELECT ol_number, sum(ol_quantity) AS sum_qty, sum(ol_amount) AS sum_amount, avg(ol_quantity) AS avg_qty, avg(ol_amount) AS avg_amount, count(*) AS count_order FROM order_line WHERE ol_delivery_d \u003e '2007-01-02 00:00:00.000000' GROUP BY ol_number ORDER BY ol_number; -- query2 SELECT su_suppkey, su_name, n_name, i_id, i_name, su_address, su_phone, su_comment FROM item, supplier, stock, nation, region, (SELECT s_i_id AS m_i_id, MIN(s_quantity) AS m_s_quantity FROM stock, supplier, nation, region WHERE MOD((s_w_id*s_i_id), 10000) = su_suppkey AND su_nationkey = n_nationkey AND n_regionkey = r_regionkey AND r_name LIKE 'Europ%' GROUP BY s_i_id) m WHERE i_id = s_i_id AND MOD((s_w_id * s_i_id), 10000) = su_suppkey AND su_nationkey = n_nationkey AND n_regionkey = r_regionkey AND i_data LIKE '%b' AND r_name LIKE 'Europ%' AND i_id=m_i_id AND s_quantity = m_s_quantity ORDER BY n_name, su_name, i_id; -- query3 SELECT ol_o_id, ol_w_id, ol_d_id, sum(ol_amount) AS revenue, o_entry_d FROM customer, new_order, oorder, order_line WHERE c_state LIKE 'A%' AND c_id = o_c_id AND c_w_id = o_w_id AND c_d_id = o_d_id AND no_w_id = o_w_id AND no_d_id = o_d_id AND no_o_id = o_id AND ol_w_id = o_w_id AND ol_d_id = o_d_id AND ol_o_id = o_id AND o_entry_d \u003e '2007-01-02 00:00:00.000000' GROUP BY ol_o_id, ol_w_id, ol_d_id, o_entry_d ORDER BY revenue DESC, o_entry_d; Test results Static result The figure above shows a performance comparison of Iceberg and Mixed-Iceberg table formats for querying static data. It can be seen from the figure that the query performance of the two table formats is similar.\nDynamic result The figure above shows a performance comparison of Iceberg 、Mixed-Iceberg and Hudi table formats for querying dynamic data. The test recorded the results of running TPC-C for different time periods.\nThe following are the specific results of each test group:\nConclusion In the case of static data, the query performance of Iceberg and Mixed-Iceberg tables is similar. Without enabling self-optimizing, the query performance of all table formats will continue to decline as dynamic data is continuously written. After enabling self-optimizing, the query performance of all table formats remains stable as dynamic data is continuously written. ","description":"","title":"Benchmark Report","uri":"https://amoro.apache.org/benchmark-report/"},{"categories":null,"content":"Download Please choose an Amoro version to download from the following tables. It is recommended you use the latest version.\nPlease verify the release with corresponding hashes(sha512), signatures and project release KEYS.\nThe instructions for checking hashes(sha512) and signatures is indicated on Verify Instructions page.\nThe latest release Version Date Source Binary Release Notes 0.8.0 2025 May 08 source (signature | sha512) amoro-bin-hadoop3 (signature | sha512) amoro-bin-hadoop2 (signature | sha512) release note All archived releases Version Date Source Binary Release Notes 0.7.1 2024 Nov 20 source (signature | sha512) amoro-bin-hadoop3 (signature | sha512) amoro-bin-hadoop2 (signature | sha512) release note 0.7.0 2024 Aug 2 source (signature | sha512) amoro-bin-hadoop3 (signature | sha512) amoro-bin-hadoop2 (signature | sha512) release note Non-Apache releases These releases were made before the Amoro project joined the ASF Incubator and have not followed the usual ASF release process.\nVersion Date Source AMS Flink Runtime Jars Spark Runtime Jars Trino Connector Release Notes 0.6.1 2024 Feb 21 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.15 Runtime Jar - Flink 1.16 Runtime Jar - Flink 1.17 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.6.0 2023 Nov 6 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.15 Runtime Jar - Flink 1.16 Runtime Jar - Flink 1.17 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.5.1 2023 Oct 10 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.5.0 2023 Aug 8 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.4.1 2023 Apr 3 source AMS - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 2.3 Runtime Jar\n- Spark 3.1 Runtime Jar Trino Connector release note 0.4.0 2022 Dec 6 source AMS - Flink 1.12 Runtime Jar\n- Flink 1.14 Runtime Jar\n- Flink 1.15 Runtime Jar - Spark 2.3 Runtime Jar\n- Spark 3.1 Runtime Jar Trino Connector release note ","description":"","title":"Download","uri":"https://amoro.apache.org/download/"},{"categories":null,"content":"Benchmark Guild This guilde introduces detailed steps for executing the benchmark to validate performance of various data lake formats.\nBy following the steps in the guild, you can learn about the analytical performance of different data lake table format. At the same time, you can flexibly adjust the test scenarios to obtain test results that better suit your actual scenario.\nDeploy testing environment Deploy by Docker With Docker-Compose, you can quickly set up an environment for performing the benchmark. The detailed steps reference: Lakehouse-benchmark.\nDeploy manually Alternatively, you can manually deploy the following components to set up the test environment：\nComponent Version Description Installation Guide MySQL 5.7+ MySQL is used to generate TPC-C data for synchronization to data lakes. MySQL Installation Guide Hadoop 2.3.7+ Hadoop is used to provide the storage for data lakes. Ambari Trino 380 Trino is used to execute TPC-H queries for Iceberg and Mixed-Iceberg format tables. Trino Installation Guide Amoro Trino Connector 0.4.0 To query Mixed-Iceberg Format tables in Trino, you need to install and configure the Amoro connector in Trino. Amoro Trino Connector Iceberg Trino Connector 0.13.0 To query Iceberg Format tables in Trino, you need to install and configure the Iceberg connector in Trino. Iceberg Trino Connector Presto 274 Presto is used to execute TPC-H queries for Hudi format tables. Presto Installation Guide Hudi Presto Connector 0.11.1 To query Iceberg Format tables in Trino, you need to install and configure the Iceberg connector in Presto. Hudi Presto Connector AMS 0.4.0 Amoro Management Service, support self-optimizing on tables during the test. AMS Installation Guide data-lake-benchmark 21 The core program of Benchmark which is responsible for generating test data, executing the testing process, and generating test results. Data Lake Benchmark lakehouse-benchmark-ingestion 1.0 Data synchronization tool based on Flink-CDC which can synchronize data from database to data lake in real-time. Lakehouse Benchmark Ingestion Benchmark steps Configure the configuration file config/mysql/sample_chbenchmark_config.xml file of program data-lake-benchmark. Fill in the information of MySQL and parameter scalefactor. scalefactor represents the number of warehouses, which controls the overall data volume. Generally, choose 10 or 100.\nGenerate static data into MySQL with command：\njava -jar lakehouse-benchmark.jar -b tpcc,chbenchmark -c config/mysql/sample_chbenchmark_config.xml --create=true --load=true Configure the configuration file config/ingestion-conf.yaml file of program lakehouse-benchmark-ingestion. Fill in the information of MySQL.\nStart the ingestion job to synchronize data form MySQL to data lake tables witch command:\njava -cp lakehouse-benchmark-ingestion-1.0-SNAPSHOT.jar com.netease.arctic.benchmark.ingestion.MainRunner -confDir [confDir] -sinkType [arctic/iceberg/hudi] -sinkDatabase [dbName] Execute TPC-H benchmark on static data with command: java -jar lakehouse-benchmark.jar -b chbenchmarkForTrino -c config/trino/trino_chbenchmark_config.xml --create=false --load=false --execute=true Execute TPC-C program to continuously write data into MYSQL witch command: java -jar lakehouse-benchmark.jar -b tpcc,chbenchmark -c config/mysql/sample_chbenchmark_config.xml --execute=true -s 5 Execute TPC-H benchmark on dynamic data with command: java -jar lakehouse-benchmark.jar -b chbenchmarkForTrino -c config/trino/trino_chbenchmark_config.xml --create=false --load=false --execute=true Obtain the benchmark results in the result directory of the data-lake-benchmark project.\nRepeat step 7 to obtain benchmark results for different points in time.\n","description":"","title":"How To Benchmark","uri":"https://amoro.apache.org/benchmark-guide/"},{"categories":null,"content":"Contributing to Amoro Thanks for your interest in the Amoro project. Contributions are welcome and are greatly appreciated! Every little effort helps, and credit will always be given.\nThis page provides some orientation and resources for getting involved with the project. It also offers recommendations for the best results when engaging with the community. We hope this will be a pleasant first experience for you and you’ll return to continue contributing.\nGet Involved If you have any questions, suggestions, or improvement ideas when using Amoro, you can participate in the Amoro community building through the following suggested channels.\nIssue Tracker - for tracking bugs, ideas, plans, etc. GitHub Discussions - second to the mailing list for anything else you want to share or ask. WeChat Group - add kllnn999 as a friend and request to join the group. Slack - You can join the Amoro community on Slack. Amoro channel is in ASF Slack workspace. Anyone with an @apache.org email address can become a full member of the ASF Slack workspace. Search Amoro channel and join it. If you don’t have an @apache.org email address, you can email to dev@amoro.apache.org to apply for an ASF Slack invitation. Then join Amoro channel. Contributing Guide See Contributing for more details on contributing to Amoro.\n","description":"","title":"How To Contribute","uri":"https://amoro.apache.org/how-to-contribute/"},{"categories":null,"content":"Welcome to Apache Amoro (incubating) Amoro community is a free, open-source community project. Anyone interested in the Amoro project can join the community and contribute to its development by becoming a part of it.\nThis document describes some guidelines for joining the Amoro community.\nMailing Lists List Name Address Subscribe Unsubscribe Archive Developer List dev@amoro.apache.org subscribe unsubscribe archive Commits List commits@amoro.apache.org subscribe unsubscribe archive Roles and Responsibilities Amoro community is composed of and operated by the following roles:\nUser Contributor Committer PPMC User Community users, as defined by Amoro, are those members of the community who need the Amoro project, either individuals or businesses.\nContributor Everyone who contributes can become an Amoro contributor. The members will provide mentorship and guidance when new contributors need assistance.\nHow to become a Contributor? 1 merged PR in this project Responsibilities and privileges Actively participate in Amoro’s project development Participate in the project’s mailing lists and other communication channels, community events (meetups, hackathons, etc.) Learn and help others learn Amoro-related technologies Be listed as an Amoro contributor Committer Committers are promoted from Contributors. They have the authority to commit to the project’s repositories and are responsible for the planning and maintenance of Amoro. They are also active members who share their knowledge with the community.\nHow to become a Committer? Have a deep understanding of Amoro’s principles and future plans Have the ability to deal with various issues that arise in the project promptly Lead a major development, write and revise related documents Be voted in by the Amoro PPMC Responsibilities and privileges Mentor and guide other members in the community Ensure continued health of the project Looking after Amoro’s trademarks and branding Writing and submitting Incubator reports Be granted write access to Amoro repositories Be listed as an Amoro Committer and featured on the Amoro official website PPMC Members PPMC members are responsible for the planning and maintenance of Amoro. They are also active members who share their knowledge with the community.\nHow to become a PPMC member? Have the ability to deal with project issues Lead project development and iterations, and steer the overall direction of the project Be voted in by the Amoro PPMC Responsibilities and privileges Mentor and guide other members in the community Ensure continued health of the project, such as code quality and test coverage Make and approve technical design decisions Define milestones and releases Vote and promote new committers and PPMC members Be listed as an Amoro PPMC member and featured on the Amoro official website ","description":"","title":"Join Community","uri":"https://amoro.apache.org/join-community/"},{"categories":null,"content":"Quickstart This guide outlines the basic process of using Amoro, allowing you to quickly experience its core features. You can choose to use either the Iceberg Format or the Mixed-Iceberg Format to complete the entire process.\nIf you are more interested in the Mixed-Hive Format or the Paimon Format, you can refer to: Mixed-Hive Format and Paimon Format. For specific information on the different formats supported by Amoro, please refer to: Table Format.\nBefore starting the quick demo, some steps are required to prepare the environment. The fastest way to get started is to use a docker-compose file that uses the apache/amoro image. To use this, you’ll need to install the Docker CLI as well as the Docker Compose CLI.\nOnce you have those, save the yaml below into a file named docker-compose.yml:\nversion: \"3\" services: minio: image: minio/minio container_name: minio environment: - MINIO_ROOT_USER=admin - MINIO_ROOT_PASSWORD=password - MINIO_DOMAIN=minio networks: amoro_network: aliases: - warehouse.minio ports: - 9001:9001 - 9000:9000 command: [ \"server\", \"/data\", \"--console-address\", \":9001\" ] mc: depends_on: - minio image: minio/mc container_name: mc networks: amoro_network: environment: - AWS_ACCESS_KEY_ID=admin - AWS_SECRET_ACCESS_KEY=password - AWS_REGION=us-east-1 entrypoint: \u003e /bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' \u0026\u0026 sleep 1; done; /usr/bin/mc rm -r --force minio/warehouse; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; tail -f /dev/null \" amoro: image: apache/amoro container_name: amoro ports: - 8081:8081 - 1630:1630 - 1260:1260 environment: - JVM_XMS=1024 networks: amoro_network: volumes: - ./amoro:/tmp/warehouse command: [\"/entrypoint.sh\", \"ams\"] tty: true stdin_open: true networks: amoro_network: driver: bridge Next, start up the docker containers with this command:\ndocker-compose up Prepare steps Create optimizer group Open http://localhost:1630 in a browser, enter admin/admin to log in to the dashboard.\nClick on Optimizing in the sidebar, choose Optimizer Groups and click Add Group button to create a new group befre creating catalog:\nCreate catalog Click on Catalogs in the sidebar, click on the + button under Catalog List to create a test catalog, and name it to demo_catalog:\nIceberg Format Mixed-Iceberg Format To use the Iceberg Format, select Type as Internal Catalog, and choose Iceberg as Table Format. To use the Mixed-Iceberg Format, select Type as Internal Catalog, and choose Mixed-Iceberg as Table Format. Start optimizers Click on Optimizing in the sidebar, select the Optimizer Group tab, and click the scale-out operation for group local.\nSet the concurrency of the optimizer to 1 and click OK.\nThen you can switch the tab to Optimizers, you can find the newly launched optimizer here.\nYou may need to wait for up to 30 seconds for the optimizer to register with AMS. Demo steps Initialize tables Click on Terminal in the sidebar, you can create the test tables here using SQL. Terminal supports executing Spark SQL statements for now.\nIceberg Format Mixed-Iceberg Format CREATE DATABASE IF NOT EXISTS db; CREATE TABLE IF NOT EXISTS db.user ( id INT, name string, ts TIMESTAMP ) USING iceberg PARTITIONED BY (days(ts)); INSERT OVERWRITE db.user VALUES (1, \"eric\", timestamp(\"2022-07-01 12:32:00\")), (2, \"frank\", timestamp(\"2022-07-02 09:11:00\")), (3, \"lee\", timestamp(\"2022-07-02 10:11:00\")); SELECT * FROM db.user; CREATE DATABASE IF NOT EXISTS db; CREATE TABLE IF NOT EXISTS db.user ( id INT, name string, ts TIMESTAMP, PRIMARY KEY(id) ) USING mixed_iceberg PARTITIONED BY (days(ts)); INSERT OVERWRITE db.user VALUES (1, \"eric\", timestamp(\"2022-07-01 12:32:00\")), (2, \"frank\", timestamp(\"2022-07-02 09:11:00\")), (3, \"lee\", timestamp(\"2022-07-02 10:11:00\")); SELECT * FROM db.user; Click on the RUN button uppon the SQL editor, and wait for the SQL query to finish executing. You can then see the query results under the SQL editor.\nMake some changes Execute the following SQL statements one by one in the Terminal:\n-- insert a few rows first INSERT INTO db.user (id, name, ts) VALUES (4, 'rock', CAST('2022-07-02 01:11:20' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (5, 'jack', CAST('2022-07-02 05:22:10' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (6, 'mars', CAST('2022-07-02 08:23:20' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (7, 'cloe', CAST('2022-07-02 08:44:50' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (8, 'smith', CAST('2022-07-02 10:52:20' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (9, 'piec', CAST('2022-07-02 11:24:30' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (10, 'vovo', CAST('2022-07-02 12:00:20' AS TIMESTAMP)); -- delete some rows then DELETE FROM db.user where id = 1; DELETE FROM db.user where id = 4; DELETE FROM db.user where id = 7; -- query the table SELECT * from db.user; Check self-optimizing As new data is written to the table, Amoro will automatically trigger self-optimizing on the table.\nClick on Tables in the sidebar, select the test table to enter the table details page, and switch to the Optimizing tab, where you can see all the self-optimizing tasks on the table.\nYou can also enter the Optimizing page through the sidebar to view the current optimizing status of all tables.\nFor more information on Self-Optimizing, please refer to: Self-optimizing\nAfter finishing the demo, you can run the following command in the directory of docker-compose.yml to stop all containers: docker-compose down ","description":"","title":"Quickstart","uri":"https://amoro.apache.org/quick-start/"},{"categories":null,"content":"How to release a new version Preparation Apache release documentation Please refer to the following link to understand the ASF release process:\nApache Release Guide Apache Release Policy Publishing Maven Artifacts Environmental requirements JDK 11 Apache Maven 3.8+ GnuPG 2.1+ Git SVN GPG signature Follow the Apache release guidelines, you need the GPG signature to sign the release version, users can also use this to determine if the downloaded version has been tampered with.\nCreate a pgp key for version signing, use \u003cyour Apache ID\u003e@apache.org as the USER-ID for the key.\nFor more details, refer to Apache Releases Signing documentation，Cryptography with OpenPGP.\nBrief process for generating a key：\nGenerate a new GPG key using gpg --gen-key, set the key length to 4096 and set it to never expire Upload the key to the public key server using gpg --keyserver keys.openpgp.org --send-key \u003cyour key id\u003e Export the public key to a text file using gpg --armor --export \u003cyour key id\u003e \u003e\u003e gpgapachekey.txt Obtain the keys of other committers for signing (optional) Add the generated key to the KEYS file (uploaded to the svn repository by the release manager) You can follow the steps below to create the GPG key:\nYou should replace amoro with your Apache ID in following guides. $ gpg --full-gen-key gpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only) (14) Existing key from card Your selection? 1 # Please enter 1 RSA keys may be between 1024 and 4096 bits long. What keysize do you want? (3072) 4096 # Please enter 4096 here Requested keysize is 4096 bits Please specify how long the key should be valid. 0 = key does not expire \u003cn\u003e = key expires in n days \u003cn\u003ew = key expires in n weeks \u003cn\u003em = key expires in n months \u003cn\u003ey = key expires in n years Key is valid for? (0) 0 # Please enter 0 Key does not expire at all Is this correct? (y/N) y # Please enter y here GnuPG needs to construct a user ID to identify your key. Real name: amoro # Please enter 'gpg real name' Email address: amoro@apache.org # Please enter your apache email address here Comment: amoro # Please enter some comments here You selected this USER-ID: \"amoro (amoro) \u003camoro@apache.org\u003e\" Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O # Please enter O here We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. # At this time, a dialog box will pop up, asking you to enter the key for this gpg. # you need to remember that it will be used in subsequent steps. ┌─────────────────────────────────────────────────────┐ │ Please enter this passphrase to │ │ protect your new key │ │ │ │ Passphrase: _______________________________________ │ │ │ │ \u003cOK\u003e \u003cCancel\u003e │ └─────────────────────────────────────────────────────┘ # Here you need to re-enter the password in the previous step. ┌─────────────────────────────────────────────────────┐ │ Please re-enter this passphrase │ │ │ │ Passphrase: _______________________________________ │ │ │ │ \u003cOK\u003e \u003cCancel\u003e │ └─────────────────────────────────────────────────────┘ gpg: key ACFB69E705016886 marked as ultimately trusted gpg: revocation certificate stored as '/root/.gnupg/openpgp-revocs.d/DC12398CCC33A5349EB9663DF9D970AB18C9EDF6.rev' public and secret key created and signed. pub rsa4096 2023-05-01 [SC] 85778A4CE4DD04B7E07813ABACFB69E705016886 uid amoro (amoro) \u003camoro@apache.org\u003e sub rsa4096 2023-05-01 [E] Then you can follow the steps below to upload the GPG key to the public server:\n$ gpg --keyid-format SHORT --list-keys /root/.gnupg/pubring.kbx ------------------------ pub rsa4096/05016886 2023-05-01 [SC] 85778A4CE4DD04B7E07813ABACFB69E705016886 uid [ultimate] amoro (amoro) \u003camoro@apache.org\u003e sub rsa4096/0C5A4E1C 2023-05-01 [E] # Send public key to keyserver via key id $ gpg --keyserver keyserver.ubuntu.com --send-key 05016886 # send key should be found in the --list-keys result # Among them, keyserver.ubuntu.com is the selected keyserver, it is recommended to use this, because the Apache Nexus verification uses this keyserver Check if the key is uploaded successfully:\n$ gpg --keyserver keyserver.ubuntu.com --recv-keys 05016886 # If the following content appears, it means success gpg: key ACFB69E705016886: \"amoro (amoro) \u003camoro@apache.org\u003e\" not changed gpg: Total number processed: 1 gpg: unchanged: 1 Add the GPG public key to the KEYS file of the Apache SVN project warehouse:\n# Add public key to KEYS in dev branch $ mkdir -p ~/amoro_svn/dev $ cd ~/amoro_svn/dev $ svn co https://dist.apache.org/repos/dist/dev/incubator/amoro $ cd ~/amoro_svn/dev/amoro # Append the KEY you generated to the file KEYS, and check if it is added correctly $ (gpg --list-sigs amoro@apache.org \u0026\u0026 gpg --export --armor amoro@apache.org) \u003e\u003e KEYS $ svn ci -m \"add gpg key for amoro\" # Add public key to KEYS in release branch $ mkdir -p ~/amoro_svn/release $ cd ~/amoro_svn/release $ svn co https://dist.apache.org/repos/dist/release/incubator/amoro/ $ cd ~/amoro_svn/release/amoro # Append the KEY you generated to the file KEYS, and check if it is added correctly $ (gpg --list-sigs amoro@apache.org \u0026\u0026 gpg --export --armor amoro@apache.org) \u003e\u003e KEYS $ svn ci -m \"add gpg key for amoro\" Maven settings During the release process, frequent access to your Apache password is required. To prevent exposure in plaintext storage, we need to encrypt it.\n# Generate master password $ mvn --encrypt-master-password \u003capache password\u003e {EM+4/TYVDXYHRbkwjjAS3mE1RhRJXJUSG8aIO5RSxuHU26rKCjuS2vG+/wMjz9te} Create the file ${user.home}/.m2/settings-security.xml and configure the password created in the previous step:\n\u003csettingsSecurity\u003e \u003cmaster\u003e{EM+4/TYVDXYHRbkwjjAS3mE1RhRJXJUSG8aIO5RSxuHU26rKCjuS2vG+/wMjz9te}\u003c/master\u003e \u003c/settingsSecurity\u003e In the maven configuration file ~/.m2/settings.xml, add the following item:\n\u003csettings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"\u003e \u003cservers\u003e \u003cserver\u003e \u003cid\u003eapache.snapshots.https\u003c/id\u003e \u003c!-- APACHE LDAP UserName --\u003e \u003cusername\u003eamoro\u003c/username\u003e \u003c!-- APACHE LDAP password (Fill in the password you just created with the command `mvn --encrypt-password \u003capache passphrase\u003e`) --\u003e \u003cpassword\u003e{/ZLaH78TWboH5IRqNv9pgU4uamuqm9fCIbw0gRWT01c=}\u003c/password\u003e \u003c/server\u003e \u003cserver\u003e \u003cid\u003eapache.releases.https\u003c/id\u003e \u003c!-- APACHE LDAP UserName --\u003e \u003cusername\u003eamoro\u003c/username\u003e \u003c!-- APACHE LDAP password (Fill in the password you just created with the command `mvn --encrypt-password \u003capache passphrase\u003e`) --\u003e \u003cpassword\u003e{/ZLaH78TWboH5IRqNv9pgU4uamuqm9fCIbw0gRWT01c=}\u003c/password\u003e \u003c/server\u003e \u003c/servers\u003e \u003cprofiles\u003e \u003cprofile\u003e \u003cid\u003eapache-release\u003c/id\u003e \u003cproperties\u003e \u003cgpg.keyname\u003e05016886\u003c/gpg.keyname\u003e \u003c!-- Use an agent: Prevents being asked for the password during the build --\u003e \u003cgpg.useagent\u003etrue\u003c/gpg.useagent\u003e \u003cgpg.passphrase\u003epassphrase for your gpg key\u003c/gpg.passphrase\u003e \u003c/properties\u003e \u003c/profile\u003e \u003c/profiles\u003e \u003c/settings\u003e Generate the final encrypted password and add it to the ~/.m2/settings.xml file:\n$ mvn --encrypt-password \u003capache password\u003e {/ZLaH78TWboH5IRqNv9pgU4uamuqm9fCIbw0gRWT01c=} Build release Cut the release branch Cut the release branch if it is not created in the Apache remote repository, if it already exists, check it out and pull the latest changes.\n$ cd ${AMORO_SOURCE_HOME} # Cut the release branch if it is not created $ git checkout -b 0.8.x # Or check it out and pull the latest changes if it is created $ git checkout 0.8.x $ git pull apache 0.8.x Change the project version to the release version in the tools/change-version.sh:\nOLD=\"0.8-SNAPSHOT\" NEW=\"0.8.0-incubating\" HERE=` basename \"$PWD\"` if [[ \"$HERE\" != \"tools\" ]]; then echo \"Please only execute in the tools/ directory\"; exit 1; fi # change version in all pom files find .. -name 'pom.xml' -type f -exec perl -pi -e 's#\u003cversion\u003e'\"$OLD\"'\u003c/version\u003e#\u003cversion\u003e'\"$NEW\"'\u003c/version\u003e#' {} \\; Then run the scripts and commit the change:\n$ cd tools $ bash change-version.sh $ cd .. $ git add * $ git commit -m \"Change project version to 0.8.0-incubating\" $ git push apache 0.8.x Create the release tag Create the release tag and push it to the Apache repo:\n$ git tag -a v0.8.0-rc1 -m \"Release Apache Amoro 0.8.0 rc1\" $ git push apache v0.8.0-rc1 Build binary and source release Build Amoro binary release with scripts:\n$ cd ${AMORO_SOURCE_HOME}/tools $ RELEASE_VERSION=0.8.0-incubating bash ./releasing/create_binary_release.sh Then build source release with scripts:\n$ cd ${AMORO_SOURCE_HOME}/tools $ RELEASE_VERSION=0.8.0-incubating bash ./releasing/create_source_release.sh Validate the source and binary packages according to the How to validate a new release guides. After that, publish the dev directory of the Apache SVN warehouse of the material package:\n$ cd ~/amoro_svn/dev/amoro $ mkdir 0.8.0-incubating-RC1 $ cp ${AMORO_SOURCE_HOME}/tools/releasing/release/* 0.8.0-incubating-RC1 $ svn add 0.8.0-incubating-RC1 $ svn commit -m \"Release Apache Amoro 0.8.0 rc1\" Release Apache Nexus Next, we will publish the required JAR files to the ​Apache Nexus​ repository to achieve the final goal of releasing them to the ​Maven Central Repository.\n$ cd ${$AMORO_SOURCE_HOME}/tools $ RELEASE_VERSION=0.8.0-incubating bash ./releasing/deploy_staging_jars.sh You can visit https://repository.apache.org/ and log in to check the publishment status. You can find the publishment process in the Staging Repositories section. You need to close the process when all jars are publised.\nVote for the new release Next, vote for the new release via email. First complete the vote within the Amoro community, and upon approval, complete the vote within the Amoro community in the Incubator community. For detailed voting guidelines, please refer to voting process.\nVote in the Amoro community Send a vote email to dev@amoro.apache.org to start the vote process in Apache Amoro community, you can take [VOTE] Release Apache Amoro(incubating) 0.8.0-incubating rc3 as an example.\nYou can validate the source and binary packages according to the How to validate a new release guides and give your vote resulut. If other developers identify critical issues, you need to cancel the current vote, wait for the fixes to be implemented, and then restart the release process with a new release candidate.\nAfter 72 hours, if there are at least 3 binding votes from Amoro PPMC members and no votes against, send the result email to celebrate the release of the version like [RESULT][VOTE] Release Apache Amoro(incubating) 0.8.0-incubating rc3.\nVote in the Incubator community Send a vote email to general@incubator.apache.org to start the vote process in Apache Incubator community, you can take [VOTE] Release Apache Amoro(incubating) 0.8.0-incubating rc3 as an example.\nAfter 72 hours, if there are at least 3 binding votes from IPMC members and no votes against, send the result email to celebrate the release of the version like [RESULT][VOTE] Release Apache Amoro(incubating) 0.8.0-incubating rc3.\nComplete the final publishing steps Migrate source and binary packages Migrate the source and binary packages to the release directory of the Apache SVN warehouse:\n$ svn mv https://dist.apache.org/repos/dist/dev/incubator/amoro/0.8.0-incubating-RC1 https://dist.apache.org/repos/dist/release/incubator/amoro/0.8.0-incubating -m \"Release Apache Amoro 0.8.0-incubating\" Publish releases in the Apache Staging repository Log in to http://repository.apache.org , log in with your Apache account Click Staging repositories on the left Select your most recently uploaded warehouse, the warehouse specified in the voting email Click the Release button above, this process will perform a series of checks It usually takes 24 hours for the warehouse to synchronize to other data sources\nSend announcement email Finally, we need to send the announcement email to these mailing lists: dev@amoro.apache.org, general@incubator.apache.org. Here is an example of an announcement email: [ANNOUNCE] Apache Amoro (Incubating) 0.8.0-incubating available.\nCongratulations! You have successfully completed all steps of the Apache Amoro release process. Thank you for your contributions!\n","description":"","title":"Release Guide","uri":"https://amoro.apache.org/release-guide/"},{"categories":null,"content":"How to validate a new release Download candidate # If there is svn locally, you can clone to the local $ svn co https://dist.apache.org/repos/dist/dev/incubator/amoro/${release_version}-${rc_version}/ # or download the material file directly $ wget https://dist.apache.org/repos/dist/dev/incubator/amoro/${release_version}-${rc_version}/ validate candidate Check GPG signature Download the KEYS and import it:\n$ curl https://downloads.apache.org/incubator/amoro/KEYS \u003e KEYS # Download KEYS $ gpg --import KEYS # Import KEYS to local Trust the KEY used in this version:\n$ gpg --edit-key xxxxxxxxxx #KEY user used in this version gpg (GnuPG) 2.2.21; Copyright (C) 2020 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Secret key is available. gpg\u003e trust #trust Please decide how far you trust this user to correctly verify other users' keys (by looking at passports, checking fingerprints from different sources, etc.) 1 = I don't know or won't say 2 = I do NOT trust 3 = I trust marginally 4 = I trust fully 5 = I trust ultimately m = back to the main menu Your decision? 5 #choose 5 Do you really want to set this key to ultimate trust? (y/N) y #choose y Check the gpg signature:\n$ for i in *.tar.gz; do echo $i; gpg --verify $i.asc $i; done Check sha512 hash # Command for Linux $ for i in *.tar.gz; do echo $i; sha512sum --check $i.sha512; done # Command for MacOS $ for i in *.tar.gz; do echo $i; shasum -a 512 -c $i.sha512; done Check the binary package Unzip the binary pakcages: apache-amoro-${AMORO_VERSION}-bin-${HADOOP_VERSION}.tar.gz:\n# Hadoop2 $ tar -xzvf apache-amoro-0.8.0-incubating-bin-hadoop2.tar.gz # Hadoop3 $ tar -xzvf apache-amoro-0.8.0-incubating-bin-hadoop3.tar.gz check as follows:\nCheck whether the package contains unnecessary files, which makes the tar package too large Folder contains the word incubating There are LICENSE and NOTICE files There is a DISCLAIMER file Check for extra files or folders, such as empty folders, etc. Check the source package Unzip the binary pakcages: apache-amoro-${AMORO_VERSION}-src.tar.gz:\n$ tar -xzvf apache-amoro-0.8.0-incubating-src.tar.gz Check as follows:\nThere are LICENSE and NOTICE files There is a DISCLAIMER file All source files have ASF license at the beginning Only source files exist, not binary files Compile from source:\n# Compile from source $ mvn clean package # Or skip the unit test $ mvn clean package -DskipTests vote for the release If all verifications pass, please vote +1 for the new release! If you find any critical issues, pleaste vote -1 for it. Thanks a lot for your work!\n","description":"","title":"Release Guide","uri":"https://amoro.apache.org/validate-release/"},{"categories":null,"content":"Roadmap This roadmap displays the workings that the Amoro community plans to complete in 2024 and their priorities. Each item is associated with a Github issue, where you can learn about its specific design and latest developments.\nPriority 1 Refactor AMS using function and process api Add metric collection for ams Support table sorting process Serverless process: automatic scaling for process resources Monitor the health status of tables Improve Mixed format Priority 2 Support Apache Hudi Support ingestion process Maintain secondary indexes on tables Collect scan metrics and improve optimizing rules based on them Enhance the management capabilities of the Paimon format ","description":"","title":"Roadmap","uri":"https://amoro.apache.org/roadmap/"}]